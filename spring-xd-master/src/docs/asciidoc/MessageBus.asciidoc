[[messagebus]]
== Message Bus Configuration

=== Introduction

This section contains additional information about configuring the Message Bus, including High Availability, SSL,
Error handling and partitioning.

[[rabbit-message-bus-high-availability-ha-configuration]]
=== Rabbit Message Bus High Availability (HA) Configuration

==== Introduction

The +RabbitMessageBus+ allows for HA configuration using normal https://www.rabbitmq.com/ha.html[RabbitMQ HA Configuration].

First, use the +addresses+ property in +servers.yml+ to include the host/port for each server in the cluster. See xref:Application-Configuration#rabbitConfig[Application Configuration].

By default, queues and exchanges declared by the bus are prefixed with +xdbus.+ (this prefix can be changed as described in xref:Application-Configuration#rabbitBusProps[Application Configuration]).

To configure the entire bus for HA, create a policy:

+rabbitmqctl set_policy ha-xdbus "^xdbus\." \'{"ha-mode":"all"}'+

==== Connection Management and HA Queues

When consuming from HA queues, there might be some performance advantage in consuming from the node that actually hosts
the queue.
Starting with _version 1.2_ it is now possible to configure the Rabbit Message Bus to do that.

CAUTION: To utilize this mechanism, the https://www.rabbitmq.com/management.html[rabbit management plugin] must be enabled on each node in the cluster.
The plugin's REST API is used to determine the location of the queue.

This feature is enabled by adding more than one node to the +spring.rabbitmq.node+ property.
See xref:Application-Configuration#rabbitConfig[RabbitMQ Configuration] for configuration details.

When a node fails and a queue is moved to one of the mirrors, the bus will automatically reconnect to the right node.

[[error-handling-message-delivery-failures]]
=== Error Handling (Message Delivery Failures)

==== RabbitMQ Message Bus

NOTE: The following applies to normally deployed streams. When direct binding between modules is being used, exceptions thrown by the consumer are thrown back to the producer.

When a consuming module (processor, sink) fails to handle a message, the bus will retry delivery based on the module (or default bus) retry configuration. The default configuration will make 3 attempts to deliver the message. The retry configuration can be modified at the bus level (in +servers.yml+), or for an individual stream/module using the deployment manifest.

When retries are exhausted, by default, messages are discarded. However, using RabbitMQ, you can configure such messages to be routed to a dead-letter exchange/dead letter queue. See the https://www.rabbitmq.com/dlx.html[RabbitMQ Documentation] for more information.

NOTE: The following configuration examples assume you are using the default bus +prefix+ used for naming rabbit elements: +"xdbus."+

Consider a stream: +stream create foo --definition "source | processor | sink"+

The first _pipe_ (by default) will be backed by a queue named +xdbus.foo.0+, the second by +xdbus.foo.1+. Messages are routed to these queues using the default exchange (with routing keys equal to the queue names).

To enable dead lettering just for this stream, first configure a policy:

+rabbitmqctl set_policy foo.DLX "^xdbus\.foo\..*" \'{"dead-letter-exchange":"foo.dlx"}' --apply-to queues+

To configure dead-lettering for all streams:

+rabbitmqctl set_policy DLX "^xdbus\..*" \'{"dead-letter-exchange":"dlx"}' --apply-to queues+

The next step is to declare the dead letter exchange, and bind dead letter queues with the appropriate routing keys.

For example, for the second "pipe" in the stream above we might bind a queue +foo.sink.dlq+ to exchange +foo.dlx+ with a routing key +xdbus.foo.1+ (remember, the original routing key was the queue name).

Now, when the sink fails to handle a message, after the configured retries are exhausted, the failed message will be routed to +foo.sink.dlq+.

There is no automated mechanism provided to move dead lettered messages back to the bus queue.


*Automatic Dead Lettering Queue Binding*

Starting with _version 1.1_, the dead letter queue and binding can be automatically configured by the system. A new property +autoBindDLQ+ has been added; it can be set at the bus level (in +servers.yml+) or using deployment properties, e.g. +--properties module.*.consumer.autoBindDLQ=true+ for all modules in the stream. When +true+, the dead letter queue will be declared (if necessary) and bound to a dead letter exchange named +xdbus.DLX+ (again, assuming the default +prefix+) using the queue name as the routing key.

In the above example, where we have queues +xdbus.foo.0+ and +xdbus.foo.1+, the system will also create +xdbus.foo.0.dlq+, bound to +xdbus.DLX+ with routing key +xdbus.foo.0+ and +xdbus.foo.1.dlq+, bound to +xdbus.DLX+ with routing key +xdbus.foo.1+.

NOTE: Starting with _version 1.2_, any queues that are deployed with `autoBindDLQ` will automatically be configured to enable dead-lettering, routing to the DLX with the proper routing key. It is no longer necessary to use a policy to set up dead-lettering when using `autoBindDLQ`.

Also, starting with _version 1.2_, the provision of dead-lettering on publish/subscribe named channels (`tap:` or `topic:`) depends on a new deployment property `durable`.
This property is similar to a JMS durable subscription to a topic and is `false` by default.
When `false` (default), the queue backing such a named channel is declared `auto-delete` and is removed when the stream is undeployed.
A DLQ will not be created for such queues.
When `true`, the queue becomes permanent (durable) and is not removed when the stream is undeployed.
Also, when `true`, the queue is eligible for DLQ provisioning, according to the `autoBindDLQ` deployment property.
`durable` can be set at the bus level, or in an individual deployment property, such as:

`stream create ticktock --definition="time --fixedDelay=5 | log" --deploy`
`stream create tttap --definition="tap:stream:ticktock > log"`
`stream deploy tttap --properties=module.log.consumer.durableSubscription=true`

==== Redis Message Bus

When Redis is the transport, the failed messages (after retries are exhausted) are +LPUSH+ed to a +LIST ERRORS:<stream>.n+ (e.g. +ERRORS:foo.1+ in the above example in the _RabbitMQ Message Bus_ section).

This is unconditional; the data in the +ERRORS LIST+ is in "bus" format; again, as with the RabbitMQ Message Bus, some external mechanism would be needed to move the data from the ERRORS LIST back to the bus's foo.1 LIST.

NOTE: When moving errored messages back to the main stream, it is important to understand that these messages contain binary data and are unlikely to survive conversion to and from +Unicode+ (such as with Java +String+ variables). If you use Java to move these messages, we recommend that you use a +RedisTemplate+ configured as follows:

    <bean id="redisTemplate"
             class="org.springframework.data.redis.core.RedisTemplate">
	  <property name="connectionFactory" ref="jedisConnectionFactory" />
      <property name="keySerializer">
        <bean
          class="org.springframework.data.redis.serializer.StringRedisSerializer" />
      </property>
      <property name="enableDefaultSerializer" value="false" />
    </bean>

or

	@Bean
	public RedisTemplate<String, byte[]> redisTemplate() {
		RedisTemplate<String, byte[]> template = new RedisTemplate<String, byte[]>();
		template.setConnectionFactory(connectionFactory());
		template.setKeySerializer(new StringRedisSerializer());
		template.setEnableDefaultSerializer(false);
		return template;
	}

This enables the message payload to be retained as +byte[]+ with no conversion; you would then use something like...

    byte[] errorEvt = redisTemplate.opsForList().rightPop(errorQueue);
    redisTemplate.opsForList().leftPush(destinationQueue, errorEvt);


If, after moving a message, you see an error such as:

    redis.RedisMessageBus$ReceivingHandler - Could not convert message: EFBFBD...

This is a sure sign that a +UTF-8 -> Unicode -> UTF-8+ conversion was performed on the message.


[[rabbitssl]]
=== Rabbit Message Bus Secure Sockets Layer (SSL)

If you wish to use SSL for communications with the RabbitMQ server, consult the https://www.rabbitmq.com/ssl.html[RabbitMQ SSL Support Documentation].

First configure the broker as described there. The message bus is a client of the broker and supports both of the described configurations for connecting clients (SSL _without certificate validation_ and _with certficate validation_).

To use SSL without certificate validation, simply set

----
spring:
  rabbitmq:
    useSSL: true
----

In +application.yml+ (and set the port(s) in the +addresses+ property appropriately).

To use SSL with certificate validation, set

----
spring:
  rabbitmq:
    useSSL: true
    sslProperties: file:path/to/secret/ssl.properties
----

The +sslProperties+ property is a Spring resource (+file:+, +classpath:+ etc) that points to a properties file, Typically, this file would be secured by the operating system (and readable by the XD container) because it contains security information. Specifically:

----
keyStore=file:/secret/client/keycert.p12
trustStore=file:/secret/trustStore
keyStore.passPhrase=secret
trustStore.passPhrase=secret
----

Where the +pkcs12+ keystore contains the client certificate and the truststore contains the server's certificate as described in the rabbit documentation. The key/trust store properties are Spring resources.

NOTE: By default, the +rabbit+ source and sink modules inherit their default configuration from the container, but it can be overridden, either using +modules.yml+ or with specific module definitions.

=== Rabbit Message Bus Batching and Compression
See xref:Application-Configuration#rabbitBusProps[RabbitMQ Message Bus Properties] for information about batching and compressing messages passing through the bus.

=== Removing RabbitMQ MessageBus Resources

When a stream or job is undeployed, the broker resources (queues, exchanges) are NOT removed from RabbitMQ.
This is due to the possibility that a stream might be being undeployed temporarily, and avoids message loss.

If you wish to completely remove these resources, a REST API is provided for this purpose. In addition, the
`SpringXDTemplate` provides a Java binding for this REST api via its `streamOperations().cleanBusResources(String name)`
and `jobOperations().cleanBusResources(String name)` APIs.

Or, you can use the REST API directly; for example:

[source]
----
curl 'http://localhost:9393/streams/clean/rabbit/foo\
     ?user=guest&pw=guest&vhost=/&busPrefix=xdbus.&adminUri=http://localhost:15672'

curl 'http://localhost:9393/jobs/clean/rabbit/bar\
    ?user=guest&pw=guest&vhost=/&busPrefix=xdbus.&adminUri=http://localhost:15672'
----

Where `foo` is the stream name and `bar` is the job name.

The stream or job name supports a simple wildcard syntax; if it ends with `*`, then all streams beginning with the
name (excluding the `*`) will be cleaned.

These operations remove the inter-module stream queues, any tap exchanges created for the stream, the job queue and
request queue for partitioned jobs.

The operation will fail if any queue currently has a consumer; similarly, the operation will fail if any exchange
has a binding. Under either condition, no changes will be made to RabbitMQ.

The following query params are supported:

* adminUri - location of the RabbitMQ Admin (default `http://localhost:15672`)
* user - admin user (default `guest`)
* pw - admin password (default `guest`)
* vhost - the vhost used for the bus resources (default `/`)
* busPrefix - the prefix used for all bus resources (default `xdbus.`)

[[kafka-bus-partitions]]
=== Kafka Message Bus Partition Control

This section describes how topic partitioning functions when using Kafka as transport.

[[kafka-partition-control]]
==== Controlling the partition count of a transport topic

The _KafkaMessageBus_ will attempt to set the number of partitions in a transport topic as _consumerCount * consumerConcurrency_,
either by creating the topic with the required number of partitions, or by repartitioning it, in case it exists.

For example, let's consider a stream with the following definition:

----
stream create ingest --definition="http | hdfs"
----

A default deployment will result in the creation of a single topic with a single partition.

----
stream deploy ingest
----

A deployment (or redeployment) of the same stream with a different module count and concurrency will result in
6 partitions, evenly distributed across the 3 module instances:

----
stream deploy ingest --properties module.hdfs.count=3,module.hdfs.concurrency=2
----

Besides relying on defaults, you can customize the number of Kafka partitions used by transport topics by indicating a
minimum value to be used by deployments (by minimum, it is understood that, if smaller than
_consumerCount * consumerConcurrency_, the latter value will be used instead).

This can be done globally by changing the `xd.messagebus.kafka.default.minPartitionCount` property in
`servers.yml`:

----
xd:
  messagebus:
    kafka:
      default:
        minPartitionCount:   5
----

This will result in creating at least 5 partitions for each transport topic.

Alternatively, and for more granular control, the property can be specified for specific deployments and modules,
through the `producer.minPartitionCount` property in the deployment manifest, as in the following example, where
10 partitions will be created:

[source]
----
stream deploy ingest --properties module.http.producer.minPartitionCount=10,module.hdfs.count=3
----

Overpartitioning can serve a number of purposes, such as load balancing and distributing data among brokers, as well as
 allowing for scaling up by increasing the number of concurrent consumers in the future.

NOTE: If the Kafka topic already exists and it already has a number of partitions larger than either `minPartitionCount`
or _consumerCount * consumerConcurrency_, its partition count will remain unchanged, and the Kafka transport will operate
with all the existing partitions.
